# RBLN / vLLM NPU 동작 점검

- **모델 산출물**: `ai/Qwen3-0.6B/` 디렉터리에 `prefill.rbln`, `decoder_batch_1.rbln` 등의 센티넬 파일이 존재해 RBLN 전용 익스포트가 준비되어 있습니다. (`python - <<'PY' ... detect: True`)
- **백엔드 기본값**: `ai/main.py`에서 `MODEL_BACKEND` 기본값이 `rbln`으로 설정되어 있어 별도 설정 없이도 NPU 경로를 사용합니다.
- **로드 경로**: `ai/server_base.py`의 `build_app()`은 `backend == "rbln"`일 때 `RBLNAutoModelForCausalLM.from_pretrained()`를 호출해 NPU용 모델을 올립니다.
- **LoRA 핫스왑**: 동일 모듈에서 `model.set_active_lora()` 호출(vLLM-RBLN PR #48)로 RBLN 백엔드에서 여러 LoRA 어댑터를 핫스왑하도록 구성되어 있습니다.
- **필수 패키지**: 현 환경에 `vllm`, `vllm_rbln`, `optimum-rbln` 패키지가 설치되어 있어 RBLN 런타임 요구사항을 충족합니다. (`pip show vllm`, `pip show vllm_rbln`, `pip show optimum-rbln`)
- **GPU 의존성 없음**: `nvidia-smi` 명령이 존재하지 않아 CUDA GPU가 없음을 확인했으며, torch 백엔드를 강제하지 않는 한 RBLN 경로가 사용됩니다.
- **실행 로그 확인**: `testrat/logs/rbln_run_20251014_233154.log`에서 실제 기동 과정을 캡처했으며, 요약 리포트는 `testrat/logs/rbln_run_20251014_233154.md` 참고.

> 실행 시 체크리스트  
> 1. `MODEL_ID`(또는 역할별 `ECO_MODEL_ID` 등)를 `Qwen3-0.6B`로 유지  
> 2. `MODEL_BACKEND`를 명시하지 않거나 `rbln`으로 지정  
> 3. `ai/main.py` 또는 `run.sh`로 서버를 기동해 세 역할(eco/firm/house)이 RBLN 모델을 공유하는지 확인  
> 4. 추가 LoRA를 쓰려면 `register_rbln_loras()` 경로에 새 어댑터를 등록  
